{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6d1b8-7e81-428a-b562-c494c7ee3f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # For nebula torch installation for A100\n",
    "!pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "#!pip install -r ./requirements.txt\n",
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e83d1-3a73-483e-898e-2d3e66bf0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "import re, time, os, shutil, json\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from monai.data import list_data_collate\n",
    "import tempfile\n",
    "import monai\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.apps import CrossValidation\n",
    "from monai.transforms.intensity.array import ScaleIntensity\n",
    "from monai.transforms import (\n",
    "    LoadImage, EnsureChannelFirst, Spacing,\n",
    "    RandFlip, Resize, EnsureType,\n",
    "    LoadImaged, EnsureChannelFirstd,\n",
    "    Resized, EnsureTyped, Compose, ScaleIntensityd, \n",
    "    AddChanneld, MapTransform, AsChannelFirstd, EnsureType, \n",
    "    Activations, AsDiscrete, RandCropByPosNegLabeld, \n",
    "    RandRotate90d, LabelToMaskd, RandFlipd, RandRotated, Spacingd, RandAffined,\n",
    "    RandShiftIntensityd, Lambdad, MaskIntensityd\n",
    ")\n",
    "from utils import get_label, to_numpy\n",
    "\n",
    "from dataset import (\n",
    "    setup_dataloaders,\n",
    "    create_datafile,\n",
    "    setup_datafiles,\n",
    "    setup_transformations\n",
    "    )\n",
    "\n",
    "import configdot\n",
    "import torch\n",
    "from monai.config import print_config\n",
    "from IPython.core.debugger import set_trace\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "\n",
    "def get_label(path):\n",
    "    '''\n",
    "    Extracts label from path, e.g.:\n",
    "    '/workspace/RawData/Features/preprocessed_data/label_bernaskoni/n16.nii.gz' -> 'n16'\n",
    "    '''\n",
    "    return path.split('/')[-1].split('.')[0]\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520904f-b42f-422e-8a6e-243cca291ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./MONAI_TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7eb035-ffe1-463f-abff-34241f6864b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MONAI_DATA_DIRECTORY'] = \"./MONAI_TMP\"\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb6a21-82f3-4648-8eac-9a3d94e539d3",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540078bf-f097-46f4-b4e5-826115eebb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configdot.parse_config('configs/config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c53e4-59df-4b79-89c8-23166ec7c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "DEVICE = config.opt.device if hasattr(config.opt, \"device\") else 0\n",
    "device = torch.device(DEVICE)\n",
    "torch.cuda.set_device(DEVICE)\n",
    "\n",
    "print('Setting GPU#:', DEVICE)\n",
    "print('Using GPU#:', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d88e1-0234-4622-a82a-64f8f90993ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = config.dataset.metadata_path\n",
    " \n",
    "scaling_dict = None\n",
    "if config.dataset.scaling_method in 'torchio':\n",
    "    scaling_dict = 'torchio'\n",
    "elif config.dataset.scaling_method in 'scale_metadata':\n",
    "\n",
    "    scaling_data_path = config.dataset.scaling_metadata_path\n",
    "    scaling_dict = np.load(scaling_data_path, allow_pickle=True).item()\n",
    "else:\n",
    "    print('Warning! no SCALING METADATA used! Applying naive independent MinMax...')\n",
    "\n",
    "    \n",
    "split_dict = np.load(metadata_path, allow_pickle=True).item()\n",
    "train_list = split_dict['train']\n",
    "val_list = split_dict['test']\n",
    "\n",
    "images_list = []\n",
    "feat_params = config.dataset.features\n",
    "\n",
    "# Flag to add mask as additional sequence to Subset\n",
    "add_mask = config.dataset.trim_background\n",
    "\n",
    "train_files, train_missing_files = create_datafile(train_list, feat_params, mask=add_mask)\n",
    "val_files, val_missing_files = create_datafile(val_list, feat_params, mask=add_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e3c4a-81c2-4fc3-b579-03796ddf7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "\n",
    "from torchio.transforms.preprocessing.intensity import histogram_standardization\n",
    "from torchio.transforms.preprocessing.intensity import z_normalization\n",
    "\n",
    "def scaling_as_torchio(data_dict, features, scaling_dict):\n",
    "    mask_bool = data_dict[\"mask\"] > 0.\n",
    "    features_ = features\n",
    "    for i, feature in enumerate(features_):\n",
    "        #  condition, beceause some features like curv, sulc, thickness - don't need in scale, however, can be done.\n",
    "        if feature not in ['blurring-t1', 'blurring-t2', 'blurring-Flair', 'cr-t2', 'cr-Flair', 'variance', 'entropy']:\n",
    "            landmarks_path = Path(f'/workspace/FCDNet/landmarks/{feature}_landmarks.npy')\n",
    "        else:\n",
    "            landmarks_path = Path(f'/workspace/FCDNet/landmarks/{feature}_False_landmarks.npy')\n",
    "        landmark =  np.load(landmarks_path)\n",
    "        #d = torch.tensor(data_dict[\"image\"][i])\n",
    "        #m = torch.tensor(mask_bool)\n",
    "        d = data_dict[\"image\"][i]\n",
    "        m = mask_bool\n",
    "        prin\n",
    "        d_n = histogram_standardization._normalize(d, landmark, m)\n",
    "        tensor = z_normalization.ZNormalization.znorm(d_n, m)\n",
    "        if tensor is not None:\n",
    "            data_dict[\"image\"][i] = tensor\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523f20e-8477-4e03-8247-645c463d608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=[\"image\", \"seg\", \"mask\"]\n",
    "train_transf = Compose(\n",
    "                        [\n",
    "                            LoadImaged(keys=keys),\n",
    "                            EnsureChannelFirstd(keys=keys),\n",
    "                            mask_transform,\n",
    "                            EnsureTyped(keys=keys, dtype=torch.float)\n",
    "                        ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c0cb0-13d4-40e3-a4c8-134d435c0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278f781-06d0-475f-995f-42980e7e6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = config.dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e94c9-f036-4061-983f-a8ab7fe70f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "data_dict = train_ds[0]\n",
    "mask_bool = data_dict[\"mask\"] > 0.\n",
    "features_ = features\n",
    "\n",
    "\n",
    "for i, feature in enumerate(features_):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20, 10))\n",
    "    #  condition, beceause some features like curv, sulc, thickness - don't need in scale, however, can be done.\n",
    "    if feature not in ['blurring-t1', 'blurring-t2', 'blurring-Flair', 'cr-t2', 'cr-Flair', 'variance', 'entropy']:\n",
    "        landmarks_path = Path(f'/workspace/FCDNet/landmarks/{feature}_landmarks.npy')\n",
    "    else:\n",
    "        landmarks_path = Path(f'/workspace/FCDNet/landmarks/{feature}_False_landmarks.npy')\n",
    "    landmark =  np.load(landmarks_path)\n",
    "    #d = torch.tensor(data_dict[\"image\"][i])\n",
    "    #m = torch.tensor(mask_bool)\n",
    "    d = data_dict[\"image\"][i]\n",
    "    #m = mask_bool\n",
    "    \n",
    "    #ax[0].hist(d[:,:,:].numpy().flatten())\n",
    "    a = ax[0].imshow(d[:,:,60].numpy())\n",
    "    plt.colorbar(a, ax=ax[0])\n",
    "    \n",
    "    d_n = histogram_standardization._normalize(d, landmark, mask_bool)\n",
    "    tensor = histogram_standardization._normalize(d, landmark, mask_bool)\n",
    "    #tensor = z_normalization.ZNormalization.znorm(d_n, mask_bool)\n",
    "    print(tensor is not None)\n",
    "    if tensor is not None:\n",
    "        data_dict[\"image\"][i] = tensor\n",
    "        print(f'{feature} normalized: \\n Min Value: {data_dict[\"image\"][i].max()} \\n Max Value: {data_dict[\"image\"][i].min()}')\n",
    "    \n",
    "    #ax[1].hist(tensor[:,:,:].numpy().flatten())\n",
    "    #b = ax[1].matshow(tensor[:,:,60].numpy(),norm=LogNorm(vmin=10e-6, vmax=2))\n",
    "    b = ax[1].imshow(tensor[:,:,60].numpy())\n",
    "\n",
    "    \n",
    "    plt.colorbar(b, ax=ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafdebf-a35f-4163-a5f7-37b5fe31ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version \n",
    "version('torchio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27189fb4-0157-47b2-b69f-1f8e02b5265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "\n",
    "interpolate = config.default.interpolate\n",
    "if interpolate:\n",
    "    spatial_size_conf = tuple(config.default.interpolation_size)\n",
    "features = config.dataset.features\n",
    "\n",
    "assert config.dataset.trim_background\n",
    "keys=[\"image\", \"seg\", \"mask\"]\n",
    "sep_k=[\"seg\", \"mask\"]\n",
    "\n",
    "if scaling_dict in 'torchio':\n",
    "    scaler = scaling_torchio_wrapper(features, scaling_dict)\n",
    "elif scaling_dict in 'scale_metadata':\n",
    "    scaler = scaling_specified_wrapper(features, scaling_dict)\n",
    "else:\n",
    "    scaler = ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0, channel_wise=True)\n",
    "\n",
    "# no-augmentation transformation\n",
    "val_transf = Compose(\n",
    "                    [\n",
    "                        LoadImaged(keys=keys),\n",
    "                        EnsureChannelFirstd(keys=keys)\n",
    "                    ] + ([Resized(keys=keys, spatial_size=spatial_size_conf)] if interpolate else []) + \\\n",
    "                    [\n",
    "                        Spacingd(keys=sep_k, pixdim=1.0),\n",
    "                        mask_transform, # zero the non-mask values\n",
    "                        binarize_target,\n",
    "                        scaler,\n",
    "                        EnsureTyped(keys=sep_k, dtype=torch.float),\n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "if config.opt.augmentation:\n",
    "\n",
    "    rand_affine_prob = config.opt.rand_affine_prob\n",
    "    rot_range = config.opt.rotation_range\n",
    "    shear_range = config.opt.shear_range\n",
    "    scale_range = config.opt.scale_range\n",
    "    translate_range = config.opt.translate_range\n",
    "\n",
    "    noise_std = config.opt.noise_std\n",
    "    flip_prob = config.opt.flip_prob\n",
    "    rand_zoom_prob = config.opt.rand_zoom_prob\n",
    "\n",
    "    # basic operations\n",
    "    transforms = [LoadImaged(keys=keys), \n",
    "                  EnsureChannelFirstd(keys=keys),\n",
    "\n",
    "                 ] + ([Resized(keys=keys, spatial_size=spatial_size_conf)] if interpolate else []) + \\\n",
    "                 [mask_transform,scaler, Spacingd(keys=sep_k, pixdim=1.0)]\n",
    "\n",
    "    if rand_affine_prob == 0 and rot_range > 0:\n",
    "        transforms.append(RandRotated(keys=keys, # apply to all!\n",
    "                            range_x=rot_range, \n",
    "                            range_y=rot_range, \n",
    "                            range_z=rot_range, \n",
    "                            prob=0.5)\n",
    "                         )\n",
    "    if flip_prob > 0:\n",
    "        transforms.append(RandFlipd(keys=keys, # apply to all!\n",
    "                                    prob=flip_prob, \n",
    "                                    spatial_axis=0))\n",
    "\n",
    "    if rand_affine_prob > 0:\n",
    "        transforms.append(RandAffined(prob=rand_affine_prob, \n",
    "                                     rotate_range=[rot_range, rot_range, rot_range], \n",
    "                                     shear_range=[shear_range, shear_range, shear_range], \n",
    "                                     translate_range=[translate_range, translate_range, translate_range], \n",
    "                                     scale_range=[scale_range, scale_range, scale_range], \n",
    "                                     padding_mode='zeros',\n",
    "                                     keys=keys # apply to all!\n",
    "                                    )\n",
    "                         )\n",
    "\n",
    "    if noise_std > 0:\n",
    "        transforms.append(RandGaussianNoised(prob=0.5, \n",
    "                                            mean=0.0, \n",
    "                                            std=noise_std, \n",
    "                                            keys=[\"image\"]\n",
    "                                           )\n",
    "                         )\n",
    "\n",
    "    if rand_zoom_prob > 0:\n",
    "        transforms.append(RandZoomd(prob=0.5, min_zoom=0.9, max_zoom=1.1, keys=keys))\n",
    "\n",
    "    # add the rest \n",
    "    transforms.extend([ # zero the non-mask values\n",
    "                        binarize_target,\n",
    "                        EnsureTyped(keys=sep_k, dtype=torch.float),\n",
    "                     ]\n",
    "                    )\n",
    "\n",
    "    train_transf = Compose(transforms)\n",
    "else:\n",
    "\n",
    "    train_transf = val_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837f201-4a8d-41ea-a1fa-ba5bf584ab29",
   "metadata": {},
   "source": [
    "### Loading train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76bed8a-43d3-41ff-9d52-4d13bf47ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# subjects_list = np.load('./metadata/metadata_fcd_nG.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7008e09-e390-4491-a322-88527ec9e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8fed0-0afd-4d7f-98d2-393c8e6e004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list = subjects_list.get('train')\n",
    "# val_list = subjects_list.get('test')\n",
    "\n",
    "# feat_params = config.dataset.features\n",
    "\n",
    "# print(len(train_list), len(val_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27563e-d6ec-4b31-a8d3-dda524db1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c692f90-0580-40c9-881b-8f5e5b6672f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configdot.parse_config('configs/config.ini')\n",
    "train_loader, val_loader = setup_dataloaders(config, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183016e-18fc-4104-87c7-a822c66241e3",
   "metadata": {},
   "source": [
    "### Transformation and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3779dc-1493-442b-9842-e5e5da610af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,check_data_sample in enumerate(train_loader):\n",
    "    \n",
    "    \n",
    "    brain_tensor, label_tensor, mask_tensor = (check_data_sample['image'],\n",
    "                                           check_data_sample['seg'],\n",
    "                                           check_data_sample['mask']\n",
    "                                           )\n",
    "\n",
    "    batch_size = brain_tensor.shape[0]\n",
    "    for k in range(batch_size):  \n",
    "\n",
    "        image = brain_tensor[k]\n",
    "        seg = label_tensor[k]\n",
    "        mask = mask_tensor[k]\n",
    "\n",
    "        num_of_channels = len(image)\n",
    "        # choose z-coord where there is a label maximum over other axes\n",
    "        label_pos = (seg > 0).sum(dim=(0,1,2)).argmax().item()\n",
    "\n",
    "        #mask = image[:1,...] <= 0 # `background mask\n",
    "\n",
    "        torch.sum(mask * image, dim=(-1,-2,-3)).type(torch.int) > 1\n",
    "\n",
    "        fig = plt.figure(\"image\", (10, 5), dpi=100)\n",
    "        \n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        ax1.imshow(image[0,:,:,label_pos], cmap='gray')\n",
    "\n",
    "        ax2 = plt.subplot(1, 2, 2)\n",
    "        ax2.hist(image[0,:,:,label_pos].cpu().numpy().flatten(), bins=100)\n",
    "        #ax2.imshow(seg[0,:,:,label_pos], alpha=0.5, cmap='jet')\n",
    "\n",
    "        # plt.colorbar()\n",
    "        # plt.xticks([])\n",
    "        # plt.yticks([])\n",
    "\n",
    "        # fig.suptitle(label, fontsize=20, color='blue')\n",
    "        # plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # if i > 6:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc144038-1fe9-4b55-8f10-2763c99c9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert config.default.interpolate\n",
    "# spatial_size_conf = tuple(config.default.interpolation_size)\n",
    "# #masked = config.dataset.trim_background\n",
    "# masked = True\n",
    "\n",
    "# def masked_transform(data_dict):\n",
    "#     data_dict[\"image\"] = data_dict[\"image\"] * data_dict[\"mask\"]\n",
    "#     return data_dict\n",
    "\n",
    "# if masked:\n",
    "#     keys=[\"image\", \"seg\", \"mask\"]\n",
    "#     sep_k=[\"seg\", \"mask\"]\n",
    "# else:\n",
    "#     keys=[\"image\", \"seg\"]\n",
    "#     sep_k=[\"seg\"]\n",
    "\n",
    "# if config.opt.augmentation:\n",
    "#     rot_range = 0.5 \n",
    "\n",
    "#     train_transf = Compose(\n",
    "#         [\n",
    "#             LoadImaged(keys=keys),\n",
    "#             EnsureChannelFirstd(keys=keys),\n",
    "#             RandRotated(keys=keys, \n",
    "#                         range_x=rot_range, \n",
    "#                         # range_y=rot_range, \n",
    "#                         # range_z=rot_range, \n",
    "#                         prob=1),\n",
    "#             RandFlipd(keys=keys, prob=0.5, spatial_axis=0),\n",
    "#             Spacingd(keys=sep_k, pixdim=1.0),\n",
    "#             Resized(keys=keys, spatial_size=spatial_size_conf),\n",
    "#             ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0, channel_wise=True),\n",
    "#             # masked_transform,\n",
    "#             EnsureTyped(keys=keys, dtype=torch.float)\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     val_transf = Compose(\n",
    "#         [\n",
    "#                 LoadImaged(keys=keys),\n",
    "#                 EnsureChannelFirstd(keys=keys),\n",
    "#                 Spacingd(keys=sep_k, pixdim=1.0),\n",
    "#                 Resized(keys=keys, spatial_size=spatial_size_conf),\n",
    "#                 ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0, channel_wise=True),\n",
    "#                 # masked_transform,\n",
    "#                 EnsureTyped(keys=keys, dtype=torch.float),\n",
    "#             ]\n",
    "#     )\n",
    "    \n",
    "# else:\n",
    "#     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e2af7-39d7-4658-9bc8-603fd5c2a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.opt.augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cb2c8-b652-4947-ab74-1e21a2b96fda",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2a423-3a8e-4f38-a71d-717f3a2e9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_batch_size = 2\n",
    "# check_dataset = Dataset(data=train_files, transform=train_transf)\n",
    "# check_loader = DataLoader(check_dataset, \n",
    "#                           batch_size=check_batch_size, \n",
    "#                           num_workers=0, \n",
    "#                           collate_fn=list_data_collate, \n",
    "#                           pin_memory=torch.cuda.is_available(),\n",
    "#                           shuffle=True\n",
    "#                           )\n",
    "\n",
    "# check_data = monai.utils.misc.first(check_loader)\n",
    "# # check_data = monai.utils.misc.first(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48d17e-941e-4682-bf4c-f527af86b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for check_data_sample in check_dataset:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8887db5-c3ab-4a3b-bfca-e91f34ebf527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_data['image'][:,1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613270a-f4f9-4eb9-8512-5f0024342337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_data['image'][:,1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b750b09-057c-4c88-8208-4f3707f7b820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for k in range(check_batch_size):\n",
    "    \n",
    "#     image = check_data['image'][k]\n",
    "#     seg = check_data['seg'][k]\n",
    "#     mask = check_data['mask'][k]\n",
    "#     label = get_label(check_dataset.data[k]['seg'])\n",
    "    \n",
    "#     print(f\"image shape: {image.shape}\")\n",
    "    \n",
    "#     num_of_channels = len(feat_params)\n",
    "#     # choose z-coord where there is a label maximum over other axes\n",
    "#     label_pos = (seg > 0).sum(dim=(0,1,2)).argmax().item()\n",
    "    \n",
    "#     #mask = image[:1,...] <= 0 # `background mask\n",
    "    \n",
    "#     torch.sum(mask * image, dim=(-1,-2,-3)).type(torch.int) > 1\n",
    "\n",
    "#     fig = plt.figure(\"image\", (10, 5), dpi=200)\n",
    "#     ax1 = plt.subplot(1, 2, 1)\n",
    "#     #plt.title(f\"{feat_params[i]}\")\n",
    "#     ax1.imshow(image[1,:,:,label_pos], cmap='gray')\n",
    "#     ax2 = plt.subplot(1, 2, 2)\n",
    "#     ax2.imshow(mask[0,:,:,label_pos], alpha=0.5)\n",
    "    \n",
    "#     # plt.colorbar()\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "        \n",
    "#     fig.suptitle(label, fontsize=20, color='blue')\n",
    "#     # plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     if k > 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec1746d-1ed6-4b48-9f2e-576ac864bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(feat_params)[torch.sum(mask * image, dim=(-1,-2,-3)).type(torch.int) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d342e-1ade-4b74-bb16-0107a37abbcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for k in range(check_batch_size):\n",
    "    \n",
    "#     image = check_data['image'][k]\n",
    "#     seg = check_data['seg'][k]\n",
    "#     label = get_label(check_dataset.data[k]['seg'])\n",
    "    \n",
    "#     print(f\"image shape: {image.shape}\")\n",
    "    \n",
    "#     num_of_channels = len(feat_params)\n",
    "#     # choose z-coord where there is a label maximum over other axes\n",
    "#     label_pos = (seg > 0).sum(dim=(0,1,2)).argmax().item()\n",
    "    \n",
    "#     mask = image[:1,...] <= 0 # `background mask\n",
    "#     torch.sum(mask * image, dim=(-1,-2,-3)).type(torch.int) > 1\n",
    "\n",
    "#     fig = plt.figure(\"image\", (5, 5), dpi=200)\n",
    "    \n",
    "#     plt.subplot(1, 1, 1)\n",
    "#     image_bin = image[-2,:,:,label_pos] > 0\n",
    "#     plt.imshow(image_bin, cmap='gray')\n",
    "#     plt.colorbar()\n",
    "#     #plt.title(f\"{feat_params[i]}, {image_bin.sum()}\")\n",
    "#     # plt.imshow(seg[0,:,:,label_pos], interpolation='none', cmap='Reds', alpha=0.3)\n",
    "#     # plt.imshow(seg[0,:,:,label_pos], interpolation='none', cmap='Reds', alpha=0.3)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "        \n",
    "#     fig.suptitle(label, fontsize=20, color='blue')\n",
    "#     # plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6702ed1-f77a-4fa3-b17b-b9df6ed3c449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figures_per_row = 6 # for visualization\n",
    "# for k in range(check_batch_size):\n",
    "    \n",
    "#     image = check_data['image'][k]\n",
    "#     seg = check_data['seg'][k]\n",
    "#     label = get_label(check_dataset.data[k]['seg'])\n",
    "    \n",
    "#     print(f\"image shape: {image.shape}\")\n",
    "    \n",
    "#     num_of_channels = len(feat_params)\n",
    "#     # choose z-coord where there is a label maximum over other axes\n",
    "#     label_pos = (seg > 0).sum(dim=(0,1,2)).argmax().item()\n",
    "    \n",
    "#     mask = image[:1,...] <= 0 # `background mask\n",
    "#     torch.sum(mask * image, dim=(-1,-2,-3)).type(torch.int) > 1\n",
    "\n",
    "#     fig = plt.figure(\"image\", (len(feat_params), 5), dpi=200)\n",
    "#     for i in range(num_of_channels):\n",
    "#         nrows = int(np.ceil(num_of_channels/figures_per_row))\n",
    "#         cols = num_of_channels%figures_per_row\n",
    "#         plt.subplot(nrows, figures_per_row, i+1)\n",
    "#         plt.title(f\"{feat_params[i]}\")\n",
    "#         plt.imshow(image[i,:,:,label_pos] > 0, cmap=\"gray\")\n",
    "#         # plt.imshow(seg[0,:,:,label_pos], interpolation='none', cmap='Reds', alpha=0.3)\n",
    "#         # plt.imshow(seg[0,:,:,label_pos], interpolation='none', cmap='Reds', alpha=0.3)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "        \n",
    "#     fig.suptitle(label, fontsize=20, color='blue')\n",
    "#     # plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27540a85-2e83-46c2-9e55-430ceebde584",
   "metadata": {},
   "source": [
    "# Check dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e33c9-6ebf-4bfe-8ea7-8e9dc0c05f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configdot.parse_config('configs/config.ini')\n",
    "# train_loader, val_loader = setup_dataloaders(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96206e91-0386-44f3-9a15-4c32545326f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_transform(data_dict):\n",
    "    data_dict[\"mask\"] = (data_dict[\"mask\"] > 0).astype(np.int)\n",
    "    data_dict[\"image\"] = data_dict[\"image\"] * (data_dict[\"mask\"])\n",
    "    return data_dict\n",
    "\n",
    "def setup_transformations(config):\n",
    "    \n",
    "    # assert False, 'Check mask mult!'\n",
    "    \n",
    "    assert config.default.interpolate\n",
    "    spatial_size_conf = tuple(config.default.interpolation_size)\n",
    "    \n",
    "    keys=[\"image\", \"seg\", \"mask\"]\n",
    "    sep_k=[\"seg\", \"mask\"]\n",
    "\n",
    "    if config.opt.augmentation:\n",
    "        rot_range = config.opt.rotation_range\n",
    "\n",
    "        train_transf = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=keys),\n",
    "                EnsureChannelFirstd(keys=keys),\n",
    "                RandRotated(keys=keys, \n",
    "                            range_x=rot_range, #rot_range, \n",
    "                            range_y=rot_range, \n",
    "                            range_z=rot_range, \n",
    "                            prob=0.5),\n",
    "                RandFlipd(keys=keys, prob=0.5, spatial_axis=0),\n",
    "                Resized(keys=keys, spatial_size=spatial_size_conf),\n",
    "                Spacingd(keys=sep_k, pixdim=1.0),\n",
    "                ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0, channel_wise=True),\n",
    "                mask_transform, \n",
    "                EnsureTyped(keys=sep_k, dtype=torch.float),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        val_transf = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=keys),\n",
    "                EnsureChannelFirstd(keys=keys),\n",
    "                Resized(keys=keys, spatial_size=spatial_size_conf),\n",
    "                Spacingd(keys=sep_k, pixdim=1.0),\n",
    "                ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0, channel_wise=True),\n",
    "                mask_transform,\n",
    "                EnsureTyped(keys=sep_k, dtype=torch.float),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    return train_transf, val_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7851165-c9bd-40b4-b195-7802335c6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = config.dataset.metadata_path\n",
    "split_dict = np.load(metadata_path, allow_pickle=True).item()   \n",
    "\n",
    "\n",
    "\n",
    "train_files, val_files = setup_datafiles(split_dict, config)\n",
    "train_transf, val_transf = setup_transformations(config)\n",
    "\n",
    "# training dataset\n",
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transf)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config.opt.train_batch_size,\n",
    "    shuffle=config.dataset.shuffle_train,\n",
    "    num_workers=0,\n",
    "    collate_fn=list_data_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# validation dataset\n",
    "val_ds = monai.data.Dataset(data=val_files, transform=val_transf)\n",
    "val_loader = DataLoader(val_ds, \n",
    "                        batch_size=config.opt.val_batch_size, \n",
    "                        num_workers=0, \n",
    "                        collate_fn=list_data_collate,\n",
    "                        shuffle=False # important not to shuffle, to ensure label correspondence\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c33e0a-551a-43e9-b3f5-d9fdab2aaa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_aug = DataLoader(monai.data.Dataset(data=val_files, transform=train_transf), \n",
    "                        batch_size=config.opt.val_batch_size, \n",
    "                        num_workers=0, \n",
    "                        collate_fn=list_data_collate,\n",
    "                        shuffle=False # important not to shuffle, to ensure label correspondence\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083a4d3-8e3f-4db8-aab7-efd0408dc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(val_loader_aug))\n",
    "mask = data['mask'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f622958-1c19-4f20-8f4e-9143fbceeecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins=50\n",
    "h1 = torch.histc(data['image'][0,0][mask > 0], min=0, max=1, bins=nbins).numpy()\n",
    "h2 = torch.histc(data['image'][0,1][mask > 0], min=0, max=1, bins=nbins).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa317f-88df-407f-8019-f6e009711751",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1e-10,1,nbins), h1, alpha=0.5) # , align='edge'\n",
    "plt.plot(np.linspace(1e-10,1,nbins), h2, alpha=0.2)\n",
    "# plt.xticks(np.arange(100)[::20], np.linspace(0,1,100)[::20])\n",
    "# plt.xticks(np.linspace(0,1,100)[::20], np.linspace(0,1,100)[::20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40ea63-a7e1-42f0-8b16-7112ff0ac9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = data['image'][0,0]\n",
    "# curv = data['image'][0,1]\n",
    "\n",
    "# plt.hist(image[mask > 0] ,bins=50, alpha=0.5)\n",
    "# plt.hist(curv[mask > 0] ,bins=50, alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a6513d-344c-431b-95e8-6acc933a0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_slice(b_ind=0, c_ind=0):\n",
    "    \n",
    "    '''\n",
    "    b_ind - batch_index\n",
    "    c_ind - channel index for `brain_tensor`\n",
    "    brain_tensor - [bs,C,1,H,W,D]\n",
    "    mask_tensor - [bs,1,1,H,W,D]\n",
    "    label_tensor - [bs,1,1,H,W,D]\n",
    "    label_tensor_predicted - [bs,1,1,H,W,D]\n",
    "    '''\n",
    "    \n",
    "    label_pos = (label_tensor[b_ind,0] > 0).sum(dim=(0,1)).argmax().item()\n",
    "    \n",
    "    fig = plt.figure(\"image\", (3*5, 5), dpi=100)\n",
    "    \n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    ax1.imshow(to_numpy(brain_tensor[b_ind,c_ind,:,:,label_pos]), cmap='gray')\n",
    "    # ax1.imshow(to_numpy(mask_tensor[b_ind,0,:,:,label_pos]), alpha=0.2, cmap='Reds')\n",
    "    ax1.imshow(to_numpy(label_tensor[b_ind,0,:,:,label_pos]), alpha=0.6, cmap='Reds')\n",
    "    \n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    ax2.imshow(to_numpy(brain_tensor[b_ind,c_ind,:,:,label_pos]), cmap='gray')\n",
    "    # ax2.imshow(to_numpy(mask_tensor[b_ind,0,:,:,label_pos]), alpha=0.2, cmap='Reds')\n",
    "    ax2.imshow(to_numpy(label_tensor_predicted[b_ind,0,:,:,label_pos]), alpha=0.6, cmap='Reds')\n",
    "    \n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    ax3.imshow(to_numpy(mask_tensor[b_ind,0,:,:,label_pos]), cmap='jet')\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecec196-4a94-42c1-a1c2-db9c9915574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_seg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10777574-45c1-4026-beb1-50adcdb38a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_tensors in val_loader_aug:\n",
    "    brain_tensor, label_tensor, mask_tensor = (\n",
    "                                                data_tensors['image'].to(device),\n",
    "                                                data_tensors['seg'].to(device),\n",
    "                                                data_tensors['mask'].to(device)\n",
    "                                                )\n",
    "label_tensor_predicted = label_tensor\n",
    "\n",
    "show_prediction_slice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c052e-6d3a-4574-9ced-dc762cd9bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,(data, data_aug) in enumerate(zip(val_loader, val_loader_aug)):\n",
    "\n",
    "    image = data['image'][0,1] \n",
    "    image_aug = data_aug['image'][0,0]\n",
    "    \n",
    "    seg = data['seg'][0,0]\n",
    "    seg_aug = data_aug['seg'][0,0]\n",
    "    \n",
    "    mask = data['mask'][0,0]\n",
    "    mask_aug = data_aug['mask'][0,0]\n",
    "    \n",
    "    label = get_label(val_loader.dataset.data[k]['seg'])\n",
    "    \n",
    "    # choose z-coord where there is a label maximum over other axes\n",
    "    label_pos = (seg > 0).sum(dim=(0,1)).argmax().item()\n",
    "    \n",
    "    fig = plt.figure(\"image\", (2*5, 5), dpi=200)\n",
    "\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.imshow(image[:,:,label_pos]) # , cmap='gray'\n",
    "    ax1.imshow(mask[:,:,label_pos], alpha=0.2, cmap='jet')\n",
    "    \n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.imshow(image_aug[:,:,label_pos]) # , cmap='gray'\n",
    "    ax2.imshow(mask_aug[:,:,label_pos], alpha=0.2, cmap='jet')\n",
    "    \n",
    "    # plt.colorbar()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "        \n",
    "    fig.suptitle(label, fontsize=20, color='blue')\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if k > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c665f8a-6604-48b9-af9c-c1e1a858b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['image'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0fdf0-898d-4e87-be4a-f97c3cf0f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to check non-zero backgrounds in dataloader\n",
    "\n",
    "# features_cumsum = torch.zeros(len(config.dataset.features))\n",
    "# for train_batch in tqdm(train_loader):\n",
    "#     features_cumsum += train_batch['image'].sum(0)[:,:5,:5,:5].sum(dim=(-1,-2,-3))\n",
    "# for val_batch in tqdm(val_loader):\n",
    "#     features_cumsum += val_batch['image'].sum(0)[:,:5,:5,:5].sum(dim=(-1,-2,-3))\n",
    "# np.array(config.dataset.features)[features_cumsum > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d76d9-f26f-424e-be3a-649411b30d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = check_data['image'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4323ec-d2c1-4e8e-85a8-f7d89a91589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = check_data['mask']\n",
    "mask.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2d792-0b2c-4833-9db3-f13669c9e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2):\n",
    "    \n",
    "    image = check_data['image'][k]\n",
    "    seg = check_data['seg'][k]\n",
    "    mask = check_data['mask'][k]\n",
    "    label = get_label(train_ds.data[k]['seg'])\n",
    "    \n",
    "    print(f\"image shape: {image.shape}\")\n",
    "    \n",
    "    # choose z-coord where there is a label maximum over other axes\n",
    "    label_pos = (seg > 0).sum(dim=(0,1,2)).argmax().item()\n",
    "    \n",
    "    fig = plt.figure(\"image\", (n_features*5, 5), dpi=200)\n",
    "    for i in range(n_features):\n",
    "        ax1 = plt.subplot(1, n_features, i+1)\n",
    "        ax1.imshow(image[i,:,:,label_pos], cmap='gray')\n",
    "        # ax2 = plt.subplot(1, 2*n_features, i+1)\n",
    "        ax1.imshow(mask[0,:,:,label_pos], alpha=0.2, cmap='jet')\n",
    "    \n",
    "    # plt.colorbar()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "        \n",
    "    fig.suptitle(label, fontsize=20, color='blue')\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "    if k > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885fe961-03fd-48ba-924c-ab4e4f0583c9",
   "metadata": {},
   "source": [
    "### Cross - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51dbf1e-79bd-4cce-99ee-4d7e9e79dc09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loaders, val_loaders, test_loader = setup_dataloaders_cv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb61de0-0e7f-4894-9f8f-0f181cb401a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@abstractmethod\n",
    "class CVDataset(ABC, CacheDataset):\n",
    "    \"\"\"\n",
    "    Base class to generate cross validation datasets.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        transform,\n",
    "        cache_num=sys.maxsize,\n",
    "        cache_rate=1.0,\n",
    "        num_workers=None,\n",
    "    ) -> None:\n",
    "        data = self._split_datalist(datalist=data)\n",
    "        CacheDataset.__init__(\n",
    "            self, data, transform, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def _split_datalist(self, datalist):\n",
    "        raise NotImplementedError(f\"Subclass {self.__class__.__name__} must implement this method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75deee0f-1a25-4f55-9f5b-2d22ade7719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_transform(data_dict):\n",
    "    data_dict[\"mask\"] = (data_dict[\"mask\"] > 0).astype(np.int)\n",
    "    data_dict[\"image\"] = data_dict[\"image\"] * (data_dict[\"mask\"])\n",
    "    return data_dict\n",
    "    \n",
    "spatial_size_conf = ([128, 128, 128])\n",
    "    \n",
    "keys=[\"image\", \"seg\", \"mask\"]\n",
    "sep_k=[\"seg\", \"mask\"]\n",
    "\n",
    "rot_range = 0.15\n",
    "\n",
    "\n",
    "train_transf = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=keys),\n",
    "                EnsureChannelFirstd(keys=keys),\n",
    "                RandRotated(keys=keys, \n",
    "                            range_x=rot_range, #rot_range, \n",
    "                            range_y=rot_range, \n",
    "                            range_z=rot_range, \n",
    "                            prob=0.5),\n",
    "                RandFlipd(keys=keys, prob=0.5, spatial_axis=0),\n",
    "                Resized(keys=keys, spatial_size=spatial_size_conf),\n",
    "                Spacingd(keys=sep_k, pixdim=1.0),\n",
    "                ScalePerfectly(keys=['image'], \n",
    "                               minv=[None, None, -4, 10, ...], \n",
    "                               maxv=[None, None, -4, 10, ...], \n",
    "                               channelwise=True)\n",
    "                # ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0, channel_wise=True),\n",
    "                mask_transform, \n",
    "                EnsureTyped(keys=sep_k, dtype=torch.float),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "val_transf = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=keys),\n",
    "                EnsureChannelFirstd(keys=keys),\n",
    "                Resized(keys=keys, spatial_size=spatial_size_conf),\n",
    "                Spacingd(keys=sep_k, pixdim=1.0),\n",
    "                ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0, channel_wise=True),\n",
    "                mask_transform,\n",
    "                EnsureTyped(keys=sep_k, dtype=torch.float),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac589dac-23d5-4a9b-acb0-a364b8ff5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 8\n",
    "folds = list(range(num))\n",
    "\n",
    "cvdataset = CrossValidation(\n",
    "    dataset_cls=CVDataset,\n",
    "    data=dataset_filepaths[0][:80],\n",
    "    nfolds=8,\n",
    "    seed=42,\n",
    "    transform=train_transf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f183e75-4137-4df9-a769-fe1157535137",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d5773-6190-4562-a327-ea7213fa26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dss = [cvdataset.get_dataset(folds=folds[0: i] + folds[(i + 1):]) for i in folds]\n",
    "val_dss = [cvdataset.get_dataset(folds=i, transform=val_transf) for i in range(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425a1c8-686f-40b7-951a-e48b4793b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [DataLoader(train_dss[i], batch_size=2, shuffle=True, num_workers=0) for i in folds]\n",
    "val_loaders = [DataLoader(val_dss[i], batch_size=1, num_workers=0) for i in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b12f37-6921-44a0-a18d-3c1e8f3cc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CacheDataset(data=dataset_filepaths[0][80:], transform=val_transf, num_workers=None)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562dd9d-16eb-4f74-abef-9e8e99f2b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e78c7-37f7-4226-b242-76aff27d1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dss[0]:\n",
    "    #print(len(i))\n",
    "    file_path = i['image_meta_dict']['filename_or_obj']\n",
    "    index = file_path.split('/')[5].split('-')[-1]\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bdb34-9d76-4afd-9305-f472c68875ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.v2v import V2VModel\n",
    "\n",
    "assert config.model.name == \"v2v\"\n",
    "best_model = V2VModel(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7c747-1e62-418a-9b2a-4482498d2ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
